{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c70f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1173d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 1\n",
    "\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.shine.com/')\n",
    "job_title = driver.find_element(By.XPATH, '//*[@id=\"webSearchBar\"]/input')\n",
    "job_title.send_keys('Data Analyst')\n",
    "\n",
    "location = driver.find_element(By.ID,'id_l')\n",
    "location.send_keys('Bangalore')\n",
    "search_button = driver.find_element(By.XPATH, '//button[@type=\"submit\"]')\n",
    "search_button.click()\n",
    "job_titles = driver.find_elements(By.XPATH, '//a[@class=\"job_title_anchor\"]')\n",
    "job_locations = driver.find_elements(By.XPATH, '//li[@class=\"w-30 mr-10 result-display-location\"]/span')\n",
    "company_names = driver.find_elements(By.XPATH, '//a[@class=\"result-display-company-name\"]')\n",
    "experience_required = driver.find_elements(By.XPATH, '//li[@class=\"w-30 mr-10 result-display-exp\"]/span')\n",
    "\n",
    "data = []\n",
    "for i in range(10):\n",
    "  job = {\n",
    "  'Job Title': job_titles[i].text,\n",
    "  'Job Location': job_locations[i].text,\n",
    "  'Company Name': company_names[i].text,\n",
    "  'Experience Required': experience_required[i].text\n",
    "  }\n",
    "  data.append(job)\n",
    "df = pd.DataFrame(data)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.shine.com/')\n",
    "job_title = driver.find_element(By.XPATH, '//*[@id=\"webSearchBar\"]/input')\n",
    "job_title.send_keys('Data Scientist')\n",
    "\n",
    "location = driver.find_element(By.ID, 'id_l')\n",
    "location.send_keys('Bangalore')\n",
    "search_button = driver.find_element(By.XPATH, '//button[@type=\"submit\"]')\n",
    "search_button.click()\n",
    "job_titles = driver.find_elements(By.XPATH, '//a[@class=\"job_title_anchor\"]')\n",
    "job_locations = driver.find_elements(By.XPATH, '//li[@class=\"w-30 mr-10 result-display-location\"]/span')\n",
    "company_names = driver.find_elements(By.XPATH, '//a[@class=\"result-display-company-name\"]')\n",
    "\n",
    "data = []\n",
    "for i in range(10):\n",
    "  job = {\n",
    "  'Job Title': job_titles[i].text,\n",
    "  'Job Location': job_locations[i].text,\n",
    "  'Company Name': company_names[i].text\n",
    "  }\n",
    "  data.append(job)\n",
    "df = pd.DataFrame(data)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "search_input = soup.find('input', {'id': 'txt_search'})\n",
    "search_input['value'] = 'Data Scientist'\n",
    "search_button = soup.find('button', {'id': 'btn_search'})\n",
    "response = requests.post(url, data={'txt_search': 'Data Scientist'})\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "location_filter = soup.find('input', {'id': 'chk_location_1'})\n",
    "location_filter['checked'] = True\n",
    "salary_filter = soup.find('input', {'id': 'chk_salary_1'})\n",
    "salary_filter['checked'] = True\n",
    "job_listings = soup.find_all('div', {'class': 'w-100'})\n",
    "data = []\n",
    "for job in job_listings[:10]:\n",
    "  title = job.find('h3').text.strip()\n",
    "  location = job.find('span', {'class': 'location'}).text.strip()\n",
    "  company = job.find('span', {'class': 'company-name'}).text.strip()\n",
    "  experience = job.find('span', {'class': 'exp'}).text.strip()\n",
    "  data.append([title, location, company, experience])\n",
    "df = pd.DataFrame(data, columns=['Job Title', 'Job Location', 'Company Name', 'Experience Required'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db06e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "search_box = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input')\n",
    "search_box.send_keys('sunglasses')\n",
    "search_box.submit()\n",
    "\n",
    "sunglasses = driver.find_elements(By.XPATH, \"//div[@class='_2kHMtA']\")\n",
    "data = []\n",
    "\n",
    "for i in range(100):\n",
    "  brand = sunglasses[i].find_element(By.XPATH, \".//div[@class='_2WkVRV']\")\n",
    "  description = sunglasses[i].find_element(By.XPATH, \".//a[@class='IRpwTa']\")\n",
    "  price = sunglasses[i].find_element(By.XPATH, \".//div[@class='_30jeq3 _1_WHN1']\")\n",
    "  \n",
    "  data.append({\n",
    "  'Brand': brand.text,\n",
    "  'ProductDescription': description.text,\n",
    "  'Price': price.text\n",
    "  })\n",
    "for item in data:\n",
    "  print(item)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef391a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "reviews = soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "for review in reviews[:100]:\n",
    "  rating = review.find('div', {'class': '_3LWZlK _1BLPMq'}).text\n",
    "  summary = review.find('p', {'class': '_2-N8zT'}).text\n",
    "  full_review = review.find('div', {'class': 't-ZTKy'}).text\n",
    "  print('Rating: ', rating)\n",
    "  print('Summary: ', summary)\n",
    "  print('Full Review: ', full_review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63191dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "search_g= driver.find_element(By.XPATH, \"//input[@type='text']\")\n",
    "search_g\n",
    "\n",
    "search_g.send_keys('sneakers')\n",
    "\n",
    "search_btn=driver.find_element(By.XPATH, \"//button[@class='L0Z3Pu']\")\n",
    "search_btn\n",
    "\n",
    "search_btn=driver.find_element(By.CLASS_NAME, 'L0Z3Pu')\n",
    "search_btn.click()\n",
    "\n",
    "B_name=[]\n",
    "Price=[]\n",
    "P_desc=[]\n",
    "Discount=[]\n",
    "\n",
    "for i in range(3):\n",
    "    b_name=driver.find_elements(By.XPATH, \"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements(By.XPATH, \"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements(By.XPATH, \"//div[@class='_25b18c']\")\n",
    "    discount=driver.find_elements(By.XPATH, \"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j  in b_name:\n",
    "        B_name.append(j.text)\n",
    "    B_name[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    \n",
    "    \n",
    "    for t in discount:\n",
    "        Discount.append(t.text)\n",
    "    Discount[:100]\n",
    "    \n",
    "print(len(B_name[:100])),print(len(Price[:100])),print(len(P_desc[:100])),print(len(Discount[:100]))\n",
    "\n",
    "sneakers_df=pd.DataFrame({})\n",
    "sneakers_df['Brand_name']=B_name[:100]\n",
    "sneakers_df['P_price']=Price[:100]\n",
    "sneakers_df['Pr_desc']=P_desc[:100]\n",
    "sneakers_df['P_discount']=Discount[:100]\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 7\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url=\" https://www.amazon.in \"\n",
    "driver.get(url)\n",
    "search_g= driver.find_element(By.XPATH, \"//input[@type='text']\")\n",
    "search_g\n",
    "search_g.send_keys('Laptop')\n",
    "search_btn=driver.find_element(By.XPATH, \"//input[@id='nav-search-submit-button']\")\n",
    "search_btn\n",
    "search_btn=driver.find_element(By.XPATH, \"//input[@id='nav-search-submit-button']\")\n",
    "search_btn.click()\n",
    "Title=[]\n",
    "Price=[]\n",
    "Rating=[]\n",
    "\n",
    "for i in range(3):\n",
    "    b_name=driver.find_elements(By.XPATH, \"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements(By.XPATH, \"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements(By.XPATH, \"//div[@class='_25b18c']\")\n",
    "    \n",
    "    \n",
    "    for j  in b_name:\n",
    "        Title.append(j.text)\n",
    "    Title[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d04040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 8\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "top_quotes_button = driver.find_element(By.LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_button.click()\n",
    "quotes = driver.find_elements(By.CSS_SELECTOR, \".title a\")\n",
    "authors = driver.find_elements(By.CSS_SELECTOR, \".author a\")\n",
    "types = driver.find_elements(By.CSS_SELECTOR, \".kw-box a\")\n",
    "\n",
    "for quote, author, quote_type in zip(quotes, authors, types):\n",
    "    print(\"Quote:\", quote.text)\n",
    "    print(\"Author:\", author.text)\n",
    "    print(\"Type of Quote:\", quote_type.text)\n",
    "    print()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02dbe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 9\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.jagranjosh.com/')\n",
    "gk_option = driver.find_element(By.LINK_TEXT, 'GK')\n",
    "gk_option.click()\n",
    "pm_option = driver.find_element(By.LINK_TEXT, 'List of all Prime Ministers of India')\n",
    "pm_option.click()\n",
    "data = []\n",
    "table = driver.find_element_by_xpath(By.XPATH, '//table[@class=\"table4\"]')\n",
    "rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "for row in rows:\n",
    "  cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "  if len(cols) == 4:\n",
    "      name = cols[0].text\n",
    "      born_dead = cols[1].text\n",
    "      term_of_office = cols[2].text\n",
    "      remarks = cols[3].text\n",
    "      data.append([name, born_dead, term_of_office, remarks])\n",
    "df = pd.DataFrame(data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 10\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "search_bar = driver.find(By.ID'search-input')\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.submit()\n",
    "\n",
    "link = driver.find_element(By.LINK_TEXT'50 Most Expensive Cars in the World')\n",
    "link.click()\n",
    "\n",
    "car_names = driver.find_elements(By.XPATH, '//div[@class=\"article-content\"]/h3')\n",
    "car_prices = driver.find_elements(By.XPATH, '//div[@class=\"article-content\"]/p')\n",
    "\n",
    "data = []\n",
    "for name, price in zip(car_names, car_prices):\n",
    "  data.append([name.text, price.text])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Car Name', 'Price'])\n",
    "print(df)\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
