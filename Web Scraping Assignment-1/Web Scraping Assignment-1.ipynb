{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "headerdf = pd.DataFrame(titles)\n",
    "print(headerdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "pres = requests.get('https://presidentofindia.nic.in/former-presidents')\n",
    "in_pres = BeautifulSoup(pres.content)\n",
    "\n",
    "name = [] \n",
    "for i in in_pres.find_all('div', class_=\"desc-sec\"):\n",
    "    name.append(i.text.replace('\\n',''))\n",
    "    \n",
    "df_pres = pd.DataFrame({'Name of President and Term of office': name })\n",
    "\n",
    "pat = '(\\D+)'\n",
    "all_names = df_pres['Name of President and Term of office'].str.extract(pat,expand=False)\n",
    "\n",
    "df1 = pd.DataFrame({'Name of President': all_names,\n",
    "                     'Term of Office': ['25th July, 2017 - 25th July 2022','25th July 2012 - 25th July 2017','25th July 2007 - 25th July 2012','July 2002 - 25th July 2007',' 25 July, 1997 - 25 July 2002','25 July, 1992 - 25 July, 1997','25 July, 1987 - 25 July, 1992','25 July, 1982 - 25 July, 1987',' 25 July, 1977 - 25 July, 1982','24 August, 1974 - 11 February, 1977','24 August, 1969 - 24 August, 1974',' 13 May, 1967 - 03 May, 1969','13 May, 1962 - 13 May , 1967','26 January, 1950 - 13 May, 1962']})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc71d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3(a)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"div\")\n",
    "print(table)\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3(b)\n",
    "url = \"https://www.icc-cricket.com/rankings/batting/mens/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3(c)\n",
    "url = \"https://www.icc-cricket.com/rankings/bowling/mens/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90545d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    news_container = soup.find('li', {'class': 'LatestNews-item'})\n",
    "\n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "\n",
    "    for article in news_container.find_all('div', {'class': 'LatestNews-headlineWrapper'}):\n",
    "        headline = article.find('a').text.strip()\n",
    "        link = \"https://www.cnbc.com\" + article.find('a')['href']\n",
    "        time = article.find('time').text.strip()\n",
    "\n",
    "        headlines.append(headline)\n",
    "        times.append(time)\n",
    "        news_links.append(link)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {'Headline': headlines, 'Time': times, 'News Link': news_links}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a7c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "articles_container = soup.find(\"div\", class_=\"sc-orwwe2-3 jOMrrY\")\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "for article in articles_container.find_all(\"div\"):\n",
    "  title = article.find(\"h3\").text.strip()\n",
    "  titles.append(title)\n",
    "  \n",
    "  author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "  authors.append(author)\n",
    "  \n",
    "  date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "  dates.append(date)\n",
    "  \n",
    "  url = article.find(\"a\")[\"href\"]\n",
    "  urls.append(url)\n",
    "\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "for name in restaurant_names:\n",
    "  restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "  cuisine_list.append(cuisine.text.strip())\n",
    "\n",
    "for location in locations:\n",
    "  location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "  rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "  image_url_list.append(image['src'])\n",
    "\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
